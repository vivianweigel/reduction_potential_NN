{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "pip install --upgrade e3nn\n",
    "conda install conda-forge::ase\n",
    "conda install anaconda::scikit-learn \n",
    "pip3 install torch torchvision torchaudio\n",
    "pip install torch-scatter -f https://data.pyg.org/whl/torch-2.5.1+cpu.html\n",
    "pip install torch_geometric\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from io import StringIO\n",
    "from typing import Dict\n",
    "import warnings\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_scatter import scatter_mean\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from ase.io import read\n",
    "from ase import Atoms\n",
    "\n",
    "from e3nn.o3 import Irreps, spherical_harmonics, Linear as E3NNLinear\n",
    "from e3nn.nn import Gate\n",
    "from e3nn.nn.models.gate_points_2101 import Convolution, smooth_cutoff, tp_path_exists\n",
    "from e3nn.math import soft_one_hot_linspace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "warnings.filterwarnings('ignore', module='ase')\n",
    "\n",
    "# --- Setup log files and plot files directories ---\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_dir = \"logs\"\n",
    "figures_dir = \"figures\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(figures_dir, exist_ok=True)\n",
    "log_filename = os.path.join(log_dir, f\"log_{current_time}.log\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "                    filename=log_filename,\n",
    "                    filemode='w')\n",
    "\n",
    "# logfile.\n",
    "class LoggerWriter:\n",
    "    def __init__(self, level):\n",
    "        self.level = level\n",
    "        self.buffer = ''\n",
    "    def write(self, message):\n",
    "        if message.strip():\n",
    "            self.level(message.strip())\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "sys.stdout = LoggerWriter(logging.info)\n",
    "\n",
    "# CustomCompose\n",
    "##############################################\n",
    "class CustomCompose(nn.Module):\n",
    "    def __init__(self, first: nn.Module, second: nn.Module):\n",
    "        super().__init__()\n",
    "        self.first = first\n",
    "        self.second = second\n",
    "        self.irreps_in = self.first.irreps_in\n",
    "        self.irreps_out = self.second.irreps_out\n",
    "\n",
    "    def forward(self, *input):\n",
    "        x = self.first(*input)\n",
    "        self.first_out = x.clone()\n",
    "        x = self.second(x)\n",
    "        self.second_out = x.clone()\n",
    "        return x\n",
    "\n",
    "\n",
    "# 1. ConstrucciÃ³n del Grafo Molecular: Crea un objeto Data de PyG a partir de un objeto ASE Atoms.\n",
    "##############################################\n",
    "class MolecularGraph:\n",
    "    def __init__(self, cutoff: float = 5.0):\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "    def build_molecular_graph(self, atoms: Atoms, type_encoding: Dict[str, int], type_onehot: torch.Tensor) -> Data:\n",
    "        symbols = [sym.strip() for sym in atoms.get_chemical_symbols()]\n",
    "        onehot_features = [type_onehot[type_encoding[sym]] for sym in symbols]\n",
    "        x = torch.stack(onehot_features, dim=0)\n",
    "        masses = torch.tensor(atoms.get_masses(), dtype=torch.float32).unsqueeze(1)\n",
    "        mass_scale = 100.0\n",
    "        masses = masses / mass_scale\n",
    "        x = torch.cat([x, masses], dim=1).contiguous()\n",
    "        pos = torch.tensor(atoms.get_positions(), dtype=torch.float32)\n",
    "        from ase.neighborlist import neighbor_list\n",
    "        i_idx, j_idx, _ = neighbor_list(\"ijS\", atoms, self.cutoff)\n",
    "        edge_index = torch.stack([torch.LongTensor(i_idx), torch.LongTensor(j_idx)], dim=0)\n",
    "        edge_vec = pos[j_idx] - pos[i_idx]\n",
    "        data = Data(x=x, pos=pos, edge_index=edge_index)\n",
    "        data.edge_vec = edge_vec\n",
    "        return data\n",
    "\n",
    "# 2. Clase Dataset: bulids a dataset from a JSON file.\n",
    "##############################################\n",
    "class MolecularDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 database_path: str = 'fd.json', \n",
    "                 target_key: str = 'reduction_potential_S1 (eV)',\n",
    "                 cutoff: float = 5.0, \n",
    "                 test_size: float = 0.2):\n",
    "        super().__init__()\n",
    "        if not os.path.exists(database_path):\n",
    "            raise FileNotFoundError(f\"File '{database_path}' not found.\")\n",
    "        with open(database_path, 'r') as f:\n",
    "            raw_db = json.load(f)\n",
    "        self.database = [entry for entry in raw_db if entry.get(target_key) is not None]\n",
    "        if len(self.database) == 0:\n",
    "            raise ValueError(f\"No valid entries found with target '{target_key}' not None.\")\n",
    "        all_symbols = set()\n",
    "        for entry in self.database:\n",
    "            xyz_string = entry[\"opt_molecule_S0\"]\n",
    "            atoms = read(StringIO(xyz_string), format='xyz')\n",
    "            all_symbols.update(atoms.get_chemical_symbols())\n",
    "        self.type_encoding = {sym: i for i, sym in enumerate(sorted(all_symbols))}\n",
    "        self.type_onehot = torch.eye(len(self.type_encoding), dtype=torch.float32)\n",
    "        self.train_idx, self.test_idx = train_test_split(range(len(self.database)), test_size=test_size, random_state=42)\n",
    "        self.graph_builder = MolecularGraph(cutoff=cutoff)\n",
    "        self.target_key = target_key\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.database)\n",
    "\n",
    "    def get(self, idx: int) -> Data:\n",
    "        entry = self.database[idx]\n",
    "        xyz_string = entry[\"opt_molecule_S0\"]\n",
    "        atoms = read(StringIO(xyz_string), format='xyz')\n",
    "        data = self.graph_builder.build_molecular_graph(atoms, self.type_encoding, self.type_onehot)\n",
    "        target_value = entry[self.target_key]\n",
    "        data.y = torch.tensor([target_value], dtype=torch.float32)\n",
    "        return data\n",
    "\n",
    "    @property\n",
    "    def train_dataset(self):\n",
    "        return [self.get(i) for i in self.train_idx]\n",
    "\n",
    "    @property\n",
    "    def test_dataset(self):\n",
    "        return [self.get(i) for i in self.test_idx]\n",
    "\n",
    "# 3. PeriodicNetwork with e3nn\n",
    "##############################################\n",
    "class PeriodicNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim: int,\n",
    "                 em_dim: int,\n",
    "                 irreps_in: str,\n",
    "                 irreps_out: str,\n",
    "                 irreps_node_attr: str,\n",
    "                 layers: int,\n",
    "                 mul: int,\n",
    "                 lmax: int,\n",
    "                 number_of_basis: int,\n",
    "                 radial_layers: int,\n",
    "                 radial_neurons: int,\n",
    "                 max_radius: float,\n",
    "                 num_neighbors: float,\n",
    "                 reduce_output: bool = True):\n",
    "        super().__init__()\n",
    "        self.em = E3NNLinear(Irreps(f\"{in_dim}x0e\"), Irreps(f\"{em_dim}x0e\"))\n",
    "        self.irreps_in = Irreps(irreps_in) if irreps_in is not None else Irreps(\"0e\")\n",
    "        self.irreps_node_attr = Irreps(irreps_node_attr) if irreps_node_attr is not None else Irreps(\"0e\")\n",
    "        self.irreps_out = Irreps(irreps_out)\n",
    "        self.irreps_hidden = Irreps([(mul, (l, p)) for l in range(lmax+1) for p in [-1, 1]])\n",
    "        self.irreps_edge_attr = Irreps.spherical_harmonics(lmax)\n",
    "        self.max_radius = max_radius\n",
    "        self.number_of_basis = number_of_basis\n",
    "        self.radial_layers = radial_layers\n",
    "        self.radial_neurons = radial_neurons\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.reduce_output = reduce_output\n",
    "\n",
    "        irreps = self.irreps_in\n",
    "        act = {1: torch.nn.functional.silu, -1: torch.tanh}\n",
    "        act_gates = {1: torch.sigmoid, -1: torch.tanh}\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(layers):\n",
    "            irreps_scalars = Irreps([\n",
    "                (mul, ir) for mul, ir in self.irreps_hidden\n",
    "                if ir.l == 0 and tp_path_exists(irreps, self.irreps_edge_attr, ir)\n",
    "            ])\n",
    "            irreps_gated = Irreps([\n",
    "                (mul, ir) for mul, ir in self.irreps_hidden\n",
    "                if ir.l > 0 and tp_path_exists(irreps, self.irreps_edge_attr, ir)\n",
    "            ])\n",
    "            gate_ir = \"0e\" if tp_path_exists(irreps, self.irreps_edge_attr, \"0e\") else \"0o\"\n",
    "            irreps_gates = Irreps([(mul, gate_ir) for mul, _ in irreps_gated])\n",
    "            gate = Gate(\n",
    "                irreps_scalars,\n",
    "                [act[ir.p] for _, ir in irreps_scalars],\n",
    "                irreps_gates,\n",
    "                [act_gates[ir.p] for _, ir in irreps_gates],\n",
    "                irreps_gated\n",
    "            )\n",
    "            conv = Convolution(\n",
    "                irreps,\n",
    "                self.irreps_node_attr,\n",
    "                self.irreps_edge_attr,\n",
    "                gate.irreps_in,\n",
    "                number_of_basis,\n",
    "                radial_layers,\n",
    "                radial_neurons,\n",
    "                num_neighbors\n",
    "            )\n",
    "            irreps = gate.irreps_out\n",
    "            self.layers.append(CustomCompose(conv, gate))\n",
    "        self.layers.append(\n",
    "            Convolution(\n",
    "                irreps,\n",
    "                self.irreps_node_attr,\n",
    "                self.irreps_edge_attr,\n",
    "                self.irreps_out,\n",
    "                number_of_basis,\n",
    "                radial_layers,\n",
    "                radial_neurons,\n",
    "                num_neighbors\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def preprocess(self, data: Data):\n",
    "        batch = data.batch if hasattr(data, 'batch') else data.pos.new_zeros(data.pos.shape[0], dtype=torch.long)\n",
    "        edge_src = data.edge_index[0]\n",
    "        edge_dst = data.edge_index[1]\n",
    "        edge_vec = data.edge_vec\n",
    "        return batch, edge_src, edge_dst, edge_vec\n",
    "\n",
    "    def forward(self, data: Data) -> torch.Tensor:\n",
    "        batch, edge_src, edge_dst, edge_vec = self.preprocess(data)\n",
    "        edge_sh = spherical_harmonics(\n",
    "            self.irreps_edge_attr, edge_vec,\n",
    "            normalize=True,\n",
    "            normalization='component'\n",
    "        )\n",
    "        edge_length = edge_vec.norm(dim=1)\n",
    "        edge_length_embedded = soft_one_hot_linspace(\n",
    "            x=edge_length,\n",
    "            start=0.0,\n",
    "            end=self.max_radius,\n",
    "            number=self.number_of_basis,\n",
    "            basis='gaussian',\n",
    "            cutoff=False\n",
    "        ).mul(self.number_of_basis ** 0.5)\n",
    "        edge_attr = smooth_cutoff(edge_length / self.max_radius)[:, None] * edge_sh\n",
    "\n",
    "        if hasattr(data, 'x') and data.x is not None:\n",
    "            x = self.em(data.x)\n",
    "        else:\n",
    "            x = data.pos.new_ones((data.pos.shape[0], self.em.out_features))\n",
    "        if hasattr(data, 'z') and data.z is not None:\n",
    "            z = data.z\n",
    "        else:\n",
    "            z = data.pos.new_ones((data.pos.shape[0], self.irreps_node_attr.dim))\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, z, edge_src, edge_dst, edge_attr, edge_length_embedded)\n",
    "        if self.reduce_output:\n",
    "            out = scatter_mean(out, batch, dim=0)\n",
    "        return out\n",
    "\n",
    "# 4. Train. evaluation and Cross-Validation.\n",
    "##############################################\n",
    "def train_periodic_network(dataset,\n",
    "                           num_epochs=30,\n",
    "                           batch_size=32,\n",
    "                           device='cpu',\n",
    "                           patience=10,\n",
    "                           **kwargs):\n",
    "    model = PeriodicNetwork(**kwargs).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    train_loader = tg.loader.DataLoader(dataset.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = tg.loader.DataLoader(dataset.test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    no_improve = 0\n",
    "    history = []\n",
    "    best_state = None\n",
    "    best_epoch = 1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            if not hasattr(batch, 'batch'):\n",
    "                batch.batch = torch.zeros(batch.num_nodes, dtype=torch.long)\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(batch).squeeze(-1)\n",
    "            loss = criterion(pred, batch.y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                if not hasattr(batch, 'batch'):\n",
    "                    batch.batch = torch.zeros(batch.num_nodes, dtype=torch.long)\n",
    "                batch = batch.to(device)\n",
    "                pred = model(batch).squeeze(-1)\n",
    "                test_loss += criterion(pred, batch.y).item()\n",
    "        test_loss /= len(test_loader)\n",
    "        scheduler.step(test_loss)\n",
    "        epoch_time = time.time() - start_time\n",
    "        logging.info(f\"Epoch {epoch+1:03d}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}, Duration = {epoch_time:.2f}s\")\n",
    "        history.append({'epoch': epoch+1, 'train_loss': train_loss, 'test_loss': test_loss, 'duration': epoch_time})\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            best_epoch = epoch+1\n",
    "            best_state = model.state_dict()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                logging.info(\"Early stopping activated\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    logging.info(f\"Training finished at epoch {best_epoch} with Test Loss = {best_loss:.4f}\")\n",
    "    return model, history\n",
    "\n",
    "def evaluate_periodic_network(model, dataset, device='cpu'):\n",
    "    model.eval()\n",
    "    criterion = nn.L1Loss()\n",
    "    test_loader = tg.loader.DataLoader(dataset.test_dataset, batch_size=1, shuffle=False)\n",
    "    test_loss = 0.0\n",
    "    preds = []\n",
    "    truths = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            if not hasattr(batch, 'batch'):\n",
    "                batch.batch = torch.zeros(batch.num_nodes, dtype=torch.long)\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            loss = criterion(pred.squeeze(-1), batch.y)\n",
    "            test_loss += loss.item()\n",
    "            preds.append(pred.item())\n",
    "            truths.append(batch.y.item())\n",
    "    test_loss /= len(test_loader)\n",
    "    logging.info(f\"MAE on test set: {test_loss:.4f}\")\n",
    "    return preds, truths, test_loss\n",
    "\n",
    "def cross_validate_periodic_network(dataset, k=5, **train_kwargs):\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    all_metrics = []\n",
    "    indices = list(range(len(dataset.database)))\n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(indices), 1):\n",
    "        logging.info(f\"--- Fold {fold} ---\")\n",
    "        dataset.train_idx = train_idx.tolist()\n",
    "        dataset.test_idx = test_idx.tolist()\n",
    "        model, history = train_periodic_network(dataset, **train_kwargs)\n",
    "        preds, truths, test_loss = evaluate_periodic_network(model, dataset, device=train_kwargs.get(\"device\", \"cpu\"))\n",
    "        metrics = compute_metrics(preds, truths)\n",
    "        metrics[\"Test Loss\"] = test_loss\n",
    "        metrics[\"Fold\"] = fold\n",
    "        logging.info(f\"Fold {fold} metrics: {metrics}\")\n",
    "        all_metrics.append(metrics)\n",
    "    avg_metrics = {k: sum(m[k] for m in all_metrics)/len(all_metrics) for k in all_metrics[0] if k != \"Fold\"}\n",
    "    logging.info(\"=== Cross-Validation Results ===\")\n",
    "    for key, value in avg_metrics.items():\n",
    "        logging.info(f\"{key}: {value:.4f}\")\n",
    "    logging.info(\"================================\")\n",
    "    return all_metrics, avg_metrics\n",
    "\n",
    "# 5. metrics y plots\n",
    "##############################################\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def compute_metrics(preds, truths):\n",
    "    preds = np.array(preds)\n",
    "    truths = np.array(truths)\n",
    "    mae = np.mean(np.abs(preds - truths))\n",
    "    rmse = np.sqrt(((preds - truths)**2).mean())\n",
    "    r2 = 1 - np.sum((preds - truths)**2) / (np.sum((truths - truths.mean())**2) + 1e-9)\n",
    "    mape = np.mean(np.abs((preds - truths) / (np.abs(truths) + 1e-9))) * 100\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'MAPE (%)': mape}\n",
    "\n",
    "def save_and_show_plot(fig, filename):\n",
    "    filepath = os.path.join(figures_dir, f\"{filename}_{current_time}.png\")\n",
    "    fig.savefig(filepath)\n",
    "    plt.close(fig)\n",
    "    logging.info(f\"Figure saved as {filepath}\")\n",
    "\n",
    "def plot_loss_history(history):\n",
    "    epochs = [h['epoch'] for h in history]\n",
    "    train_loss = [h['train_loss'] for h in history]\n",
    "    test_loss  = [h['test_loss'] for h in history]\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    sns.lineplot(x=epochs, y=train_loss, marker=\"o\", label=\"Train Loss\", ax=ax)\n",
    "    sns.lineplot(x=epochs, y=test_loss, marker=\"s\", label=\"Test Loss\", ax=ax)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss (MAE)')\n",
    "    ax.set_title('Loss Curves')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    save_and_show_plot(fig, \"loss_history\")\n",
    "\n",
    "def plot_parity(preds, truths):\n",
    "    preds = np.array(preds)\n",
    "    truths = np.array(truths)\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    sns.scatterplot(x=truths, y=preds, edgecolor='k', s=80, alpha=0.7, ax=ax)\n",
    "    min_val = min(truths.min(), preds.min())\n",
    "    max_val = max(truths.max(), preds.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', label='Identity')\n",
    "    ax.set_xlabel('True Value')\n",
    "    ax.set_ylabel('Predicted Value')\n",
    "    ax.set_title('Parity Plot')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    save_and_show_plot(fig, \"parity_plot\")\n",
    "\n",
    "def plot_error_histogram(preds, truths, bins=20):\n",
    "    errors = np.array(preds) - np.array(truths)\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    sns.histplot(errors, bins=bins, kde=True, color='purple', edgecolor='black', ax=ax)\n",
    "    ax.set_xlabel('Error (Predicted - True)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Error Histogram')\n",
    "    plt.tight_layout()\n",
    "    save_and_show_plot(fig, \"error_histogram\")\n",
    "\n",
    "def plot_target_distribution(dataset):\n",
    "    train_targets = [data.y.item() for data in dataset.train_dataset]\n",
    "    test_targets  = [data.y.item() for data in dataset.test_dataset]\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "    sns.histplot(train_targets, bins=20, color='blue', edgecolor='black', kde=True, ax=ax1)\n",
    "    ax1.set_title('Training Target Distribution')\n",
    "    ax1.set_xlabel('Target Value')\n",
    "    ax1.set_ylabel('Count')\n",
    "    sns.histplot(test_targets, bins=20, color='green', edgecolor='black', kde=True, ax=ax2)\n",
    "    ax2.set_title('Test Target Distribution')\n",
    "    ax2.set_xlabel('Target Value')\n",
    "    ax2.set_ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    save_and_show_plot(fig, \"target_distribution\")\n",
    "\n",
    "def plot_cv_line_metrics(all_metrics):\n",
    "    df = pd.DataFrame(all_metrics)\n",
    "    metrics_to_plot = [col for col in df.columns if col != \"Fold\"]\n",
    "    num_metrics = len(metrics_to_plot)\n",
    "    fig, axes = plt.subplots(1, num_metrics, figsize=(6*num_metrics, 5))\n",
    "    if num_metrics == 1:\n",
    "        axes = [axes]\n",
    "    for ax, metric in zip(axes, metrics_to_plot):\n",
    "        sns.pointplot(x=\"Fold\", y=metric, data=df, ax=ax, markers=\"o\", linestyles=\"--\")\n",
    "        ax.set_title(f\"{metric} per Fold\")\n",
    "        ax.set_xlabel(\"Fold\")\n",
    "        ax.set_ylabel(metric)\n",
    "        mean_val = df[metric].mean()\n",
    "        ax.axhline(mean_val, color=\"red\", linestyle=\"--\", label=f\"Average = {mean_val:.2f}\")\n",
    "        ax.legend()\n",
    "    plt.tight_layout()\n",
    "    save_and_show_plot(fig, \"cv_line_metrics\")\n",
    "\n",
    "def plot_model_statistics(model):\n",
    "    weights = []\n",
    "    grads = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            weights.append(param.detach().cpu().numpy().flatten())\n",
    "            if param.grad is not None:\n",
    "                grads.append(param.grad.detach().cpu().numpy().flatten())\n",
    "    if weights:\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "        sns.histplot(np.concatenate(weights), bins=50, color='blue', kde=True, ax=ax1)\n",
    "        ax1.set_title(\"Weights Distribution\")\n",
    "        ax1.set_xlabel(\"Value\")\n",
    "        ax1.set_ylabel(\"Frequency\")\n",
    "        if grads:\n",
    "            sns.histplot(np.concatenate(grads), bins=50, color='red', kde=True, ax=ax2)\n",
    "            ax2.set_title(\"Gradients Distribution\")\n",
    "            ax2.set_xlabel(\"Value\")\n",
    "            ax2.set_ylabel(\"Frequency\")\n",
    "        plt.tight_layout()\n",
    "        save_and_show_plot(fig, \"model_statistics\")\n",
    "    else:\n",
    "        logging.info(\"No parameters found for visualization.\")\n",
    "\n",
    "# --- Model Accuracy\"\n",
    "def compute_model_accuracy(preds, truths, tol=0.1):\n",
    "    \"\"\"\n",
    "    Define la \"accuracy\" del modelo como el porcentaje de predicciones cuya\n",
    "    diferencia relativa es menor que tol (por defecto 10%).\n",
    "    \"\"\"\n",
    "    preds = np.array(preds)\n",
    "    truths = np.array(truths)\n",
    "    accuracy = np.mean(np.abs(preds - truths) / (np.abs(truths) + 1e-9) < tol)\n",
    "    return accuracy * 100\n",
    "\n",
    "def plot_model_accuracy(accuracy, tol=0.1):\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.bar([\"Model Accuracy\"], [accuracy], color='teal')\n",
    "    ax.set_ylim([0,100])\n",
    "    ax.set_ylabel(\"Accuracy (%)\")\n",
    "    ax.set_title(f\"Model Accuracy (Relative Error < {tol*100:.0f}%)\")\n",
    "    plt.tight_layout()\n",
    "    save_and_show_plot(fig, \"model_accuracy\")\n",
    "\n",
    "def print_summary(history, metrics, dataset, hyperparams: dict, target_name=\"reduction_potential_S1 (eV)\"):\n",
    "    total_data = len(dataset)\n",
    "    train_data = len(dataset.train_dataset)\n",
    "    test_data  = len(dataset.test_dataset)\n",
    "    best_entry = min(history, key=lambda x: x['test_loss'])\n",
    "    summary = (\n",
    "        f\"Execution Date: {datetime.datetime.now()}\\n\"\n",
    "        \"===== Training Summary =====\\n\"\n",
    "        f\"Total data points         : {total_data}\\n\"\n",
    "        f\"Training data points      : {train_data}\\n\"\n",
    "        f\"Test data points          : {test_data}\\n\"\n",
    "        f\"Target variable           : {target_name}\\n\"\n",
    "        f\"Hyperparameters:\\n\"\n",
    "    )\n",
    "    for key, value in hyperparams.items():\n",
    "        summary += f\"   {key}: {value}\\n\"\n",
    "    summary += (\n",
    "        f\"Best Epoch                : {best_entry['epoch']} (Duration: {best_entry.get('duration', 0):.2f}s)\\n\"\n",
    "        f\"Train Loss (MAE)          : {best_entry['train_loss']:.4f}\\n\"\n",
    "        f\"Test Loss (MAE)           : {best_entry['test_loss']:.4f}\\n\"\n",
    "        \"----- Final Test Metrics -----\\n\"\n",
    "    )\n",
    "    for k, v in metrics.items():\n",
    "        summary += f\"{k:12}: {v:.4f}\\n\"\n",
    "    summary += \"================================\\n\"\n",
    "    summary_filename = os.path.join(log_dir, f\"summary_{current_time}.txt\")\n",
    "    with open(summary_filename, \"w\") as f:\n",
    "        f.write(summary)\n",
    "    logging.info(summary)\n",
    "    print(summary)\n",
    "\n",
    "def evaluate_full_dataset(model, dataset, device='cpu'):\n",
    "    model.eval()\n",
    "    all_data = dataset.train_dataset + dataset.test_dataset\n",
    "    loader = tg.loader.DataLoader(all_data, batch_size=1, shuffle=False)\n",
    "    preds, truths = [], []\n",
    "    criterion = nn.L1Loss()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            if not hasattr(data, 'batch'):\n",
    "                data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "            data = data.to(device)\n",
    "            pred = model(data).squeeze(-1)\n",
    "            loss = criterion(pred, data.y)\n",
    "            total_loss += loss.item()\n",
    "            preds.append(pred.item())\n",
    "            truths.append(data.y.item())\n",
    "    total_loss /= len(loader)\n",
    "    metrics = compute_metrics(preds, truths)\n",
    "    logging.info(\"\\n===== Full Dataset Evaluation =====\")\n",
    "    logging.info(f\"Total data points: {len(all_data)}\")\n",
    "    logging.info(f\"Average MAE     : {total_loss:.4f}\")\n",
    "    for k, v in metrics.items():\n",
    "        logging.info(f\"{k:12}: {v:.4f}\")\n",
    "    logging.info(\"===================================\\n\")\n",
    "    print(\"\\n===== Full Dataset Evaluation =====\")\n",
    "    print(f\"Total data points: {len(all_data)}\")\n",
    "    print(f\"Average MAE     : {total_loss:.4f}\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k:12}: {v:.4f}\")\n",
    "    print(\"===================================\\n\")\n",
    "    return preds, truths, total_loss, metrics\n",
    "\n",
    "# 6. Molecular graph.\n",
    "##############################################\n",
    "def plot_structure_and_graph(dataset, index: int):\n",
    "    entry = dataset.database[index]\n",
    "    xyz_string = entry[\"opt_molecule_S0\"]\n",
    "    atoms = read(StringIO(xyz_string), format='xyz')\n",
    "    data = dataset.graph_builder.build_molecular_graph(atoms, dataset.type_encoding, dataset.type_onehot)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    pos_atoms = atoms.get_positions()\n",
    "    axs[0].scatter(pos_atoms[:, 0], pos_atoms[:, 1], s=100, color='skyblue', edgecolor='k')\n",
    "    for i, sym in enumerate(atoms.get_chemical_symbols()):\n",
    "        axs[0].annotate(sym, (pos_atoms[i, 0], pos_atoms[i, 1]), textcoords=\"offset points\", xytext=(5, 5))\n",
    "    axs[0].set_title(\"Molecular Structure (XY Projection)\")\n",
    "    axs[0].set_xlabel(\"X\")\n",
    "    axs[0].set_ylabel(\"Y\")\n",
    "    pos_graph = data.pos.cpu().numpy()\n",
    "    axs[1].scatter(pos_graph[:, 0], pos_graph[:, 1], s=100, color='lightgreen', edgecolor='k')\n",
    "    edge_index = data.edge_index.cpu().numpy()\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        start = pos_graph[edge_index[0, i]]\n",
    "        end = pos_graph[edge_index[1, i]]\n",
    "        axs[1].plot([start[0], end[0]], [start[1], end[1]], color='gray', alpha=0.7)\n",
    "    axs[1].set_title(\"Molecular Graph\")\n",
    "    axs[1].set_xlabel(\"X\")\n",
    "    axs[1].set_ylabel(\"Y\")\n",
    "    plt.tight_layout()\n",
    "    save_and_show_plot(fig, \"structure_and_graph\")\n",
    "\n",
    "# 7. Script Principal\n",
    "##############################################\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = MolecularDataset(\n",
    "        database_path='test.json',\n",
    "        target_key='reduction_potential_S1 (eV)',\n",
    "        cutoff=5.0,\n",
    "        test_size=0.3\n",
    "    )\n",
    "    in_dim_actual = len(dataset.type_encoding) + 1\n",
    "    logging.info(f\"Detected one-hot dimension (with mass): {in_dim_actual}\")\n",
    "    logging.info(f\"Total data points: {len(dataset)} | Train: {len(dataset.train_dataset)} | Test: {len(dataset.test_dataset)}\")\n",
    "\n",
    "    hyperparams = {\n",
    "        \"num_epochs\": 20,\n",
    "        \"batch_size\": 32,\n",
    "        \"device\": \"cpu\",\n",
    "        \"patience\": 10,\n",
    "        \"in_dim\": in_dim_actual,\n",
    "        \"em_dim\": 64,\n",
    "        \"irreps_in\": \"64x0e\",\n",
    "        \"irreps_out\": \"1x0e\",\n",
    "        \"irreps_node_attr\": \"64x0e\",\n",
    "        \"layers\": 3,\n",
    "        \"mul\": 32,\n",
    "        \"lmax\": 1,\n",
    "        \"number_of_basis\": 20,\n",
    "        \"radial_layers\": 1,\n",
    "        \"radial_neurons\": 100,\n",
    "        \"max_radius\": 5.0,\n",
    "        \"num_neighbors\": 12.0,\n",
    "        \"reduce_output\": True\n",
    "    }\n",
    "\n",
    "    model, history = train_periodic_network(dataset, **hyperparams)\n",
    "    torch.save(model.state_dict(), \"trained_model.pt\")\n",
    "    logging.info(\"Trained model saved as 'trained_model.pt'.\")\n",
    "\n",
    "    preds, truths, test_mae = evaluate_periodic_network(model, dataset, device=hyperparams[\"device\"])\n",
    "    logging.info(f\"Final MAE on test set: {test_mae:.4f}\")\n",
    "    metrics = compute_metrics(preds, truths)\n",
    "    print_summary(history, metrics, dataset, hyperparams, target_name=\"homo (eV)\")\n",
    "\n",
    "    model_accuracy = compute_model_accuracy(preds, truths, tol=0.1)\n",
    "    logging.info(f\"Model Accuracy (relative error < 10%): {model_accuracy:.2f}%\")\n",
    "    plot_model_accuracy(model_accuracy, tol=0.1)\n",
    "    # (5-fold)\n",
    "    all_cv_metrics, avg_cv_metrics = cross_validate_periodic_network(dataset, k=5, **{\n",
    "        \"num_epochs\": 30,\n",
    "        \"batch_size\": 32,\n",
    "        \"device\": \"cpu\",\n",
    "        \"patience\": 30,\n",
    "        \"in_dim\": in_dim_actual,\n",
    "        \"em_dim\": 64,\n",
    "        \"irreps_in\": \"64x0e\",\n",
    "        \"irreps_out\": \"1x0e\",\n",
    "        \"irreps_node_attr\": \"64x0e\",\n",
    "        \"layers\": 3,\n",
    "        \"mul\": 32,\n",
    "        \"lmax\": 1,\n",
    "        \"number_of_basis\": 10,\n",
    "        \"radial_layers\": 1,\n",
    "        \"radial_neurons\": 100,\n",
    "        \"max_radius\": 5.0,\n",
    "        \"num_neighbors\": 12.0,\n",
    "        \"reduce_output\": True\n",
    "    })\n",
    "    plot_cv_line_metrics(all_cv_metrics)\n",
    "    full_preds, full_truths, full_loss, full_metrics = evaluate_full_dataset(model, dataset, device=hyperparams[\"device\"])\n",
    "\n",
    "    plot_loss_history(history)\n",
    "    plot_parity(preds, truths)\n",
    "    plot_error_histogram(preds, truths, bins=20)\n",
    "    plot_target_distribution(dataset)\n",
    "    plot_model_statistics(model)\n",
    "    plot_structure_and_graph(dataset, index=0)\n",
    "\n",
    "    logging.info(\"Trained Model:\")\n",
    "    logging.info(str(model))\n",
    "    print(\"Trained Model:\")\n",
    "    print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
